{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "_BvZTjSYaY24"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1: First Part"
      ],
      "metadata": {
        "id": "AHXDQWgIZmSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Hyperparameters....\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "num_classes = 3\n",
        "patch_size = 7\n",
        "hidden_dim = 128\n",
        "mask_ratio = 0.5\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "def sample_mnist(data, classes, num_samples=100):\n",
        "    indices = []\n",
        "    for cls in classes:\n",
        "        cls_indices = [i for i, (img, label) in enumerate(data) if label == cls]\n",
        "        sampled_indices = random.sample(cls_indices, num_samples)\n",
        "        indices.extend(sampled_indices)\n",
        "    return Subset(data, indices)\n",
        "\n",
        "classes = [0, 1, 2]\n",
        "train_subset = sample_mnist(train_data, classes)\n",
        "test_subset = sample_mnist(test_data, classes)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ViT Block....\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.o = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / self.head_dim ** 0.5\n",
        "        attn_probs = attn_scores.softmax(dim=-1)\n",
        "        attended_values = (attn_probs @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        return self.o(attended_values)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, hidden_dim):\n",
        "        super(ViTBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.ffn = FeedForward(dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.attn(self.norm1(x))\n",
        "        x = x + attn_output\n",
        "        ffn_output = self.ffn(self.norm2(x))\n",
        "        x = x + ffn_output\n",
        "        return x\n",
        "\n",
        "# Masking function....\n",
        "def mask_tokens(images, mask_ratio=0.5):\n",
        "    B, C, H, W = images.shape\n",
        "    patch_size = 7\n",
        "    num_patches = (H // patch_size) * (W // patch_size)\n",
        "    num_masked = int(num_patches * mask_ratio)\n",
        "\n",
        "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    patches = patches.contiguous().view(B, C, -1, patch_size, patch_size)\n",
        "\n",
        "    masked_patches = patches.clone()\n",
        "    for i in range(B):\n",
        "        mask_indices = random.sample(range(patches.size(2)), num_masked)\n",
        "        masked_patches[i, :, mask_indices, :, :] = 0  # Set masked patches to 0...\n",
        "\n",
        "    masked_images = masked_patches.view(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
        "    masked_images = masked_images.permute(0, 1, 2, 4, 3, 5).contiguous().view(B, C, H, W)\n",
        "\n",
        "    return masked_images\n",
        "\n",
        "# MAE with ViT....\n",
        "class MAE_ViT(nn.Module):\n",
        "    def __init__(self, num_classes=3, dim=128, num_heads=8, hidden_dim=256, patch_size=7):\n",
        "        super(MAE_ViT, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_embed = nn.Conv2d(in_channels=1, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            ViTBlock(dim, num_heads, hidden_dim),\n",
        "            ViTBlock(dim, num_heads, hidden_dim)\n",
        "        )\n",
        "        self.reconstruction_layer = nn.Linear(dim, patch_size * patch_size)\n",
        "        self.classification_layer = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x, pretrain=True):\n",
        "        B, C, H, W = x.shape\n",
        "        patches = self.patch_embed(x).view(B, -1, 128)\n",
        "        tokens = self.transformer_blocks(patches)\n",
        "        if pretrain:\n",
        "            return tokens, patches  # Return tokens and original patches for MAE loss...\n",
        "        else:\n",
        "            return tokens.mean(dim=1)  # Use mean pooling for classification...\n",
        "\n",
        "def mae_pretrain(model, train_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, _ in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            masked_images = mask_tokens(images)\n",
        "            tokens, patches = model(masked_images, pretrain=True)\n",
        "            # Apply L2 loss on masked tokens only....\n",
        "            loss = F.mse_loss(tokens, patches)  # Modify this to apply loss on masked tokens...\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MAE_ViT().to(device)\n",
        "mae_pretrain(model, train_loader)\n",
        "\n",
        "def finetune(model, train_loader, test_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device), pretrain=False)\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss / len(train_loader)}\")\n",
        "\n",
        "    # Test the model...\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images.to(device), pretrain=False)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "finetune(model, train_loader, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpmlYrx9vSmb",
        "outputId": "61078900-2204-43fb-f603-e20226730686"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.06600709445774555\n",
            "Epoch 2/20, Loss: 0.021137029770761727\n",
            "Epoch 3/20, Loss: 0.011758691165596247\n",
            "Epoch 4/20, Loss: 0.007916843285784125\n",
            "Epoch 5/20, Loss: 0.005966625642031431\n",
            "Epoch 6/20, Loss: 0.004752375325188041\n",
            "Epoch 7/20, Loss: 0.003998328046873212\n",
            "Epoch 8/20, Loss: 0.0035275065572932364\n",
            "Epoch 9/20, Loss: 0.003173700114712119\n",
            "Epoch 10/20, Loss: 0.0028679018141701818\n",
            "Epoch 11/20, Loss: 0.002574163652025163\n",
            "Epoch 12/20, Loss: 0.0023936241399496795\n",
            "Epoch 13/20, Loss: 0.002202636655420065\n",
            "Epoch 14/20, Loss: 0.0020497368648648264\n",
            "Epoch 15/20, Loss: 0.0019097160897217692\n",
            "Epoch 16/20, Loss: 0.001777665107510984\n",
            "Epoch 17/20, Loss: 0.001656846550758928\n",
            "Epoch 18/20, Loss: 0.0015050343121401966\n",
            "Epoch 19/20, Loss: 0.001409724703989923\n",
            "Epoch 20/20, Loss: 0.0013280821382068097\n",
            "Epoch 1/20, Train Loss: 2.6212555170059204\n",
            "Epoch 2/20, Train Loss: 0.9580486059188843\n",
            "Epoch 3/20, Train Loss: 0.5852215141057968\n",
            "Epoch 4/20, Train Loss: 0.251011623442173\n",
            "Epoch 5/20, Train Loss: 0.12925284579396248\n",
            "Epoch 6/20, Train Loss: 0.11283745151013136\n",
            "Epoch 7/20, Train Loss: 0.07071083420887589\n",
            "Epoch 8/20, Train Loss: 0.030054230801761152\n",
            "Epoch 9/20, Train Loss: 0.02549038513097912\n",
            "Epoch 10/20, Train Loss: 0.01357497381977737\n",
            "Epoch 11/20, Train Loss: 0.013857905275654048\n",
            "Epoch 12/20, Train Loss: 0.00314845455577597\n",
            "Epoch 13/20, Train Loss: 0.0030519248568452893\n",
            "Epoch 14/20, Train Loss: 0.0018341465969569982\n",
            "Epoch 15/20, Train Loss: 0.0013567032845458015\n",
            "Epoch 16/20, Train Loss: 0.0011250326046138071\n",
            "Epoch 17/20, Train Loss: 0.0010544975695665926\n",
            "Epoch 18/20, Train Loss: 0.000777496281079948\n",
            "Epoch 19/20, Train Loss: 0.0007232065981952474\n",
            "Epoch 20/20, Train Loss: 0.0005668716199579649\n",
            "Test Accuracy: 99.00%\n",
            "Confusion Matrix:\n",
            " [[ 99   0   1]\n",
            " [  0 100   0]\n",
            " [  2   0  98]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Part"
      ],
      "metadata": {
        "id": "tiA7iZnpbAZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "\n",
        "patch_size = 7\n",
        "temperature = 0.5\n",
        "\n",
        "# Vision Transformer (ViT) with InfoNCE loss...\n",
        "class ViT_InfoNCE(nn.Module):\n",
        "    def __init__(self, dim=128, num_heads=8, hidden_dim=256, num_classes=3, temperature=temperature):\n",
        "        super(ViT_InfoNCE, self).__init__()\n",
        "        self.patch_embed = nn.Conv2d(in_channels=1, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))  # CLS token....\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 17, dim))  # 16 patches + 1 CLS token...\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            ViTBlock(dim, num_heads, hidden_dim),\n",
        "            ViTBlock(dim, num_heads, hidden_dim)\n",
        "        )\n",
        "        self.classifier = nn.Linear(dim, num_classes)  # For fine-tuning...\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, x, train_infoNCE=True):\n",
        "        B, C, H, W = x.shape\n",
        "        patches = self.patch_embed(x).view(B, -1, 128)  # [B, 16, dim]...\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, dim]...\n",
        "        tokens = torch.cat((cls_tokens, patches), dim=1)  # Append CLS token....\n",
        "\n",
        "        tokens = tokens + self.pos_embed\n",
        "\n",
        "        x = self.transformer_blocks(tokens)\n",
        "\n",
        "        if train_infoNCE:\n",
        "            # InfoNCE loss applied to CLS token......\n",
        "            return x[:, 0]\n",
        "        else:\n",
        "            # For classification....\n",
        "            return self.classifier(x[:, 0])\n",
        "\n",
        "def infoNCE_loss(cls_token_output, temperature=temperature):\n",
        "    B = cls_token_output.size(0)\n",
        "    cls_token_output = F.normalize(cls_token_output, dim=-1)  # Normalize CLS token....\n",
        "    similarity_matrix = cls_token_output @ cls_token_output.T  # Cosine similarity....\n",
        "    similarity_matrix = similarity_matrix / temperature\n",
        "\n",
        "    labels = torch.arange(B).to(device)\n",
        "    loss = F.cross_entropy(similarity_matrix, labels)\n",
        "    return loss\n",
        "\n",
        "def pretrain_infoNCE(model, train_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, _ in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            cls_token_output = model(images.to(device), train_infoNCE=True)\n",
        "            loss = infoNCE_loss(cls_token_output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, InfoNCE Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "def finetune(model, train_loader, test_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device), train_infoNCE=False)\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images.to(device), train_infoNCE=False)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "model = ViT_InfoNCE().to(device)\n",
        "pretrain_infoNCE(model, train_loader)\n",
        "\n",
        "finetune(model, train_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNBJUCVp_7B9",
        "outputId": "0a6d3bc2-0bf5-4eda-8468-ea2d833e29cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, InfoNCE Loss: 1.9629568696022033\n",
            "Epoch 2/20, InfoNCE Loss: 1.7736137509346008\n",
            "Epoch 3/20, InfoNCE Loss: 1.7321180701255798\n",
            "Epoch 4/20, InfoNCE Loss: 1.7227694511413574\n",
            "Epoch 5/20, InfoNCE Loss: 1.6957173466682434\n",
            "Epoch 6/20, InfoNCE Loss: 1.6895554900169372\n",
            "Epoch 7/20, InfoNCE Loss: 1.6965280771255493\n",
            "Epoch 8/20, InfoNCE Loss: 1.6996080219745635\n",
            "Epoch 9/20, InfoNCE Loss: 1.67894247174263\n",
            "Epoch 10/20, InfoNCE Loss: 1.661945289373398\n",
            "Epoch 11/20, InfoNCE Loss: 1.6459826350212097\n",
            "Epoch 12/20, InfoNCE Loss: 1.6488050758838653\n",
            "Epoch 13/20, InfoNCE Loss: 1.6480465352535247\n",
            "Epoch 14/20, InfoNCE Loss: 1.6484094440937043\n",
            "Epoch 15/20, InfoNCE Loss: 1.6637329041957856\n",
            "Epoch 16/20, InfoNCE Loss: 1.649157840013504\n",
            "Epoch 17/20, InfoNCE Loss: 1.655005806684494\n",
            "Epoch 18/20, InfoNCE Loss: 1.6447826504707337\n",
            "Epoch 19/20, InfoNCE Loss: 1.6483118951320648\n",
            "Epoch 20/20, InfoNCE Loss: 1.6523225009441376\n",
            "Epoch 1/20, Train Loss: 0.3856382817029953\n",
            "Epoch 2/20, Train Loss: 0.0511058266973123\n",
            "Epoch 3/20, Train Loss: 0.017157281440449878\n",
            "Epoch 4/20, Train Loss: 0.023185022216057404\n",
            "Epoch 5/20, Train Loss: 0.012325006454193498\n",
            "Epoch 6/20, Train Loss: 0.0008017196709261043\n",
            "Epoch 7/20, Train Loss: 0.0009611170215066522\n",
            "Epoch 8/20, Train Loss: 0.00018964643513754708\n",
            "Epoch 9/20, Train Loss: 0.00010506044536668923\n",
            "Epoch 10/20, Train Loss: 6.79898166708881e-05\n",
            "Epoch 11/20, Train Loss: 6.103583164076553e-05\n",
            "Epoch 12/20, Train Loss: 5.719481268897653e-05\n",
            "Epoch 13/20, Train Loss: 5.188811228435952e-05\n",
            "Epoch 14/20, Train Loss: 4.937821613566484e-05\n",
            "Epoch 15/20, Train Loss: 3.484932185529033e-05\n",
            "Epoch 16/20, Train Loss: 2.7186738543605314e-05\n",
            "Epoch 17/20, Train Loss: 2.7754114307754206e-05\n",
            "Epoch 18/20, Train Loss: 3.08763749217178e-05\n",
            "Epoch 19/20, Train Loss: 3.332167561893584e-05\n",
            "Epoch 20/20, Train Loss: 2.371746786593576e-05\n",
            "Test Accuracy: 99.00%\n",
            "Confusion Matrix:\n",
            " [[100   0   0]\n",
            " [  0 100   0]\n",
            " [  2   1  97]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Third Part"
      ],
      "metadata": {
        "id": "BAsWi3_McxMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.5\n",
        "mae_weight = 0.8\n",
        "infoNCE_weight = 0.2\n",
        "\n",
        "# Vision Transformer (ViT) with combined MAE and InfoNCE loss.....\n",
        "class ViT_MAE_InfoNCE(nn.Module):\n",
        "    def __init__(self, dim=128, num_heads=8, hidden_dim=256, num_classes=3, temperature=0.5):\n",
        "        super(ViT_MAE_InfoNCE, self).__init__()\n",
        "        self.patch_embed = nn.Conv2d(in_channels=1, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 17, dim))\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            ViTBlock(dim, num_heads, hidden_dim),\n",
        "            ViTBlock(dim, num_heads, hidden_dim)\n",
        "        )\n",
        "        self.decoder = nn.Linear(dim, dim)  # For MAE, output same dimension as patch embeddings....\n",
        "        self.classifier = nn.Linear(dim, num_classes)  # For fine-tuning....\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, x, mask_ratio=0.5, pretrain=True):\n",
        "        B, C, H, W = x.shape\n",
        "        patches = self.patch_embed(x).view(B, -1, 128)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        tokens = torch.cat((cls_tokens, patches), dim=1)\n",
        "\n",
        "        tokens = tokens + self.pos_embed\n",
        "\n",
        "        # Pass through transformer....\n",
        "        x = self.transformer_blocks(tokens)\n",
        "\n",
        "        if pretrain:\n",
        "            # Masking for MAE..\n",
        "            num_patches = tokens.shape[1] - 1\n",
        "            num_masked = int(mask_ratio * num_patches)\n",
        "            mask_indices = torch.randperm(num_patches)[:num_masked]\n",
        "\n",
        "            # Masked patches for MAE...\n",
        "            masked_patches = patches.clone()\n",
        "            masked_patches[:, mask_indices, :] = 0  # Set masked patches to 0...\n",
        "\n",
        "            # MAE Loss: Reconstruction of masked patches...\n",
        "            decoded_patches = self.decoder(x[:, 1:])  # Decode to patch embeddings....\n",
        "            mae_loss = F.mse_loss(decoded_patches[:, mask_indices], patches[:, mask_indices])\n",
        "\n",
        "            # InfoNCE Loss: Applied to CLS token...\n",
        "            cls_token_output = x[:, 0]  # Extract CLS token...\n",
        "            infoNCE_loss = self.compute_infoNCE_loss(cls_token_output)\n",
        "\n",
        "            # Weighted combined loss...\n",
        "            combined_loss = mae_weight * mae_loss + infoNCE_weight * infoNCE_loss\n",
        "            return combined_loss\n",
        "        else:\n",
        "            # For classification\n",
        "            return self.classifier(x[:, 0])\n",
        "\n",
        "    def compute_infoNCE_loss(self, cls_token_output):\n",
        "        B = cls_token_output.size(0)\n",
        "        cls_token_output = F.normalize(cls_token_output, dim=-1)\n",
        "        similarity_matrix = cls_token_output @ cls_token_output.T\n",
        "        similarity_matrix = similarity_matrix / self.temperature\n",
        "\n",
        "        labels = torch.arange(B).to(device)\n",
        "        loss = F.cross_entropy(similarity_matrix, labels)\n",
        "        return loss\n",
        "\n",
        "def pretrain_MAE_InfoNCE(model, train_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, _ in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(images.to(device), pretrain=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Combined Loss (MAE + InfoNCE): {total_loss/len(train_loader)}\")\n",
        "\n",
        "def finetune(model, train_loader, test_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device), pretrain=False)\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images.to(device), pretrain=False)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "model = ViT_MAE_InfoNCE().to(device)\n",
        "pretrain_MAE_InfoNCE(model, train_loader)\n",
        "finetune(model, train_loader, test_loader)\n"
      ],
      "metadata": {
        "id": "i0zdHnL3k3d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a10262-2c21-4620-bd59-e7239d6b746a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Combined Loss (MAE + InfoNCE): 0.44800223410129547\n",
            "Epoch 2/20, Combined Loss (MAE + InfoNCE): 0.38868720531463624\n",
            "Epoch 3/20, Combined Loss (MAE + InfoNCE): 0.3696760326623917\n",
            "Epoch 4/20, Combined Loss (MAE + InfoNCE): 0.3638882577419281\n",
            "Epoch 5/20, Combined Loss (MAE + InfoNCE): 0.35507997423410415\n",
            "Epoch 6/20, Combined Loss (MAE + InfoNCE): 0.3482196643948555\n",
            "Epoch 7/20, Combined Loss (MAE + InfoNCE): 0.34474830478429797\n",
            "Epoch 8/20, Combined Loss (MAE + InfoNCE): 0.3458032086491585\n",
            "Epoch 9/20, Combined Loss (MAE + InfoNCE): 0.3455981820821762\n",
            "Epoch 10/20, Combined Loss (MAE + InfoNCE): 0.3439487859606743\n",
            "Epoch 11/20, Combined Loss (MAE + InfoNCE): 0.34252172112464907\n",
            "Epoch 12/20, Combined Loss (MAE + InfoNCE): 0.34050036370754244\n",
            "Epoch 13/20, Combined Loss (MAE + InfoNCE): 0.3399395927786827\n",
            "Epoch 14/20, Combined Loss (MAE + InfoNCE): 0.33940733373165133\n",
            "Epoch 15/20, Combined Loss (MAE + InfoNCE): 0.33965698182582854\n",
            "Epoch 16/20, Combined Loss (MAE + InfoNCE): 0.3383944734930992\n",
            "Epoch 17/20, Combined Loss (MAE + InfoNCE): 0.33610949516296384\n",
            "Epoch 18/20, Combined Loss (MAE + InfoNCE): 0.33455678522586824\n",
            "Epoch 19/20, Combined Loss (MAE + InfoNCE): 0.3344637602567673\n",
            "Epoch 20/20, Combined Loss (MAE + InfoNCE): 0.3340968877077103\n",
            "Epoch 1/20, Train Loss: 0.41131226494908335\n",
            "Epoch 2/20, Train Loss: 0.037910232227295636\n",
            "Epoch 3/20, Train Loss: 0.0011891151283634827\n",
            "Epoch 4/20, Train Loss: 0.0005126359610585496\n",
            "Epoch 5/20, Train Loss: 9.72888547948969e-05\n",
            "Epoch 6/20, Train Loss: 5.886275248485617e-05\n",
            "Epoch 7/20, Train Loss: 9.037218543426207e-05\n",
            "Epoch 8/20, Train Loss: 3.422024456085637e-05\n",
            "Epoch 9/20, Train Loss: 3.708544677465398e-05\n",
            "Epoch 10/20, Train Loss: 2.8812881964768167e-05\n",
            "Epoch 11/20, Train Loss: 2.2385712509276345e-05\n",
            "Epoch 12/20, Train Loss: 1.9833281157843885e-05\n",
            "Epoch 13/20, Train Loss: 2.0982691421522758e-05\n",
            "Epoch 14/20, Train Loss: 1.9032865748158655e-05\n",
            "Epoch 15/20, Train Loss: 2.2153226200316568e-05\n",
            "Epoch 16/20, Train Loss: 1.9322574735269883e-05\n",
            "Epoch 17/20, Train Loss: 1.8399746386421613e-05\n",
            "Epoch 18/20, Train Loss: 1.4911678817952634e-05\n",
            "Epoch 19/20, Train Loss: 1.7022707652358804e-05\n",
            "Epoch 20/20, Train Loss: 1.3024317786403116e-05\n",
            "Test Accuracy: 98.00%\n",
            "Confusion Matrix:\n",
            " [[ 98   0   2]\n",
            " [  0 100   0]\n",
            " [  2   2  96]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fourth Part"
      ],
      "metadata": {
        "id": "3I6-Y4WvfIL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "num_classes = 3\n",
        "patch_size = 4  # Smaller patch size for video...\n",
        "hidden_dim = 128\n",
        "mask_ratio = 0.8  # 80% tokens are discarded...\n",
        "temperature = 0.07\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "def sample_mnist(data, classes, num_samples=100):\n",
        "    indices = []\n",
        "    for cls in classes:\n",
        "        cls_indices = [i for i, (img, label) in enumerate(data) if label == cls]\n",
        "        sampled_indices = random.sample(cls_indices, num_samples)\n",
        "        indices.extend(sampled_indices)\n",
        "    return Subset(data, indices)\n",
        "\n",
        "classes = [0, 1, 2]\n",
        "train_subset = sample_mnist(train_data, classes)\n",
        "test_subset = sample_mnist(test_data, classes)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Video MAE Model\n",
        "class VideoMAE(nn.Module):\n",
        "    def __init__(self, dim=128, num_heads=8, hidden_dim=256, num_classes=3, patch_size=4, mask_ratio=0.8):\n",
        "        super(VideoMAE, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))  # CLS token...\n",
        "\n",
        "        # Compute the correct number of patches from the image dimensions...\n",
        "        self.num_patches_per_frame = (28 // patch_size) * (28 // patch_size)  # MNIST 28x28 images...\n",
        "        self.num_patches = 3 * self.num_patches_per_frame  # 3 frames\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim))  # Positional embeddings for all tokens + CLS token...\n",
        "\n",
        "        self.patch_embed = nn.Conv2d(in_channels=1, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            ViTBlock(dim, num_heads, hidden_dim),\n",
        "            ViTBlock(dim, num_heads, hidden_dim)\n",
        "        )\n",
        "        self.decoder = nn.Linear(dim, dim)  # Ensure the decoder output matches the patch embedding size (128 dimensions)....\n",
        "        self.classifier = nn.Linear(dim, num_classes)  # Classifier for fine-tuning...\n",
        "\n",
        "    def forward(self, frames, mask_ratio=0.8, pretrain=True):\n",
        "        B, T, C, H, W = frames.shape  # B: batch size, T: number of frames, C: channels, H/W: height/width....\n",
        "        patches = []\n",
        "\n",
        "        # Tokenize each frame\n",
        "        for t in range(T):\n",
        "            patch = self.patch_embed(frames[:, t]).view(B, -1, 128)  # [B, num_patches_per_frame, dim]....\n",
        "            patches.append(patch)\n",
        "\n",
        "        patches = torch.cat(patches, dim=1)  # Combine tokens from all 3 frames [B, num_patches, dim]...\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, dim]...\n",
        "        tokens = torch.cat((cls_tokens, patches), dim=1)  # [B, num_patches + 1, dim]....\n",
        "\n",
        "        # Ensure that positional embeddings are correctly matched to tokens\n",
        "        tokens = tokens + self.pos_embed[:, :tokens.size(1), :]\n",
        "\n",
        "        if pretrain:\n",
        "            # Randomly shuffle and mask tokens...\n",
        "            num_tokens = tokens.shape[1] - 1  # Exclude CLS token...\n",
        "            num_masked = int(mask_ratio * num_tokens)\n",
        "            shuffle_indices = torch.randperm(num_tokens)\n",
        "            mask_indices = shuffle_indices[:num_masked]\n",
        "            remaining_indices = shuffle_indices[num_masked:]  # Keep the remaining tokens\n",
        "\n",
        "            masked_tokens = tokens.clone()\n",
        "            masked_tokens[:, mask_indices, :] = 0\n",
        "\n",
        "            # Encoder...\n",
        "            encoded_tokens = self.transformer_blocks(masked_tokens)\n",
        "\n",
        "            # Inverse shuffling before decoder\n",
        "            unmasked_tokens = encoded_tokens[:, 1:].clone()  # Remove CLS token\n",
        "            unmasked_tokens[:, remaining_indices, :] = encoded_tokens[:, 1:][:, remaining_indices, :]  # Inverse shuffle\n",
        "\n",
        "            # Decoder: Reconstruct masked tokens....\n",
        "            decoded_tokens = self.decoder(unmasked_tokens)  # Match original patch embedding size (128)...\n",
        "\n",
        "            l1_loss = F.l1_loss(decoded_tokens[:, mask_indices], patches[:, mask_indices])\n",
        "            return l1_loss\n",
        "\n",
        "        else:\n",
        "            # For classification during fine-tuning....\n",
        "            encoded_tokens = self.transformer_blocks(tokens)\n",
        "            return self.classifier(encoded_tokens[:, 0])\n",
        "\n",
        "\n",
        "# Pretraining with Video MAE\n",
        "def pretrain_video_MAE(model, train_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, _ in train_loader:\n",
        "            # Simulate 3 frames by duplicating image tensor 3 times...\n",
        "            frames = images.unsqueeze(1).repeat(1, 3, 1, 1, 1)  # [B, 3, C, H, W]...\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(frames.to(device), pretrain=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Pretraining Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "\n",
        "def finetune_video_MAE(model, train_loader, test_loader, num_epochs=20):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            # Simulate 3 frames...\n",
        "            frames = images.unsqueeze(1).repeat(1, 3, 1, 1, 1)  # [B, 3, C, H, W]....\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(frames.to(device), pretrain=False)\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            frames = images.unsqueeze(1).repeat(1, 3, 1, 1, 1)  # [B, 3, C, H, W]...\n",
        "            outputs = model(frames.to(device), pretrain=False)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "model = VideoMAE().to(device)\n",
        "pretrain_video_MAE(model, train_loader)\n",
        "finetune_video_MAE(model, train_loader, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "kvkqbdbFk3cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3c8b71-9153-441b-ff38-39780a7ec492"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Pretraining Loss: 0.18979522978004656\n",
            "Epoch 2/20, Pretraining Loss: 0.13230857723637632\n",
            "Epoch 3/20, Pretraining Loss: 0.10349317914561222\n",
            "Epoch 4/20, Pretraining Loss: 0.0842678668467622\n",
            "Epoch 5/20, Pretraining Loss: 0.06977845002946101\n",
            "Epoch 6/20, Pretraining Loss: 0.058081918249004764\n",
            "Epoch 7/20, Pretraining Loss: 0.048894419089743964\n",
            "Epoch 8/20, Pretraining Loss: 0.041238336774863694\n",
            "Epoch 9/20, Pretraining Loss: 0.035019256566700185\n",
            "Epoch 10/20, Pretraining Loss: 0.02993740249229105\n",
            "Epoch 11/20, Pretraining Loss: 0.02602270980806727\n",
            "Epoch 12/20, Pretraining Loss: 0.02256613754128155\n",
            "Epoch 13/20, Pretraining Loss: 0.019423122762849455\n",
            "Epoch 14/20, Pretraining Loss: 0.01706473298959042\n",
            "Epoch 15/20, Pretraining Loss: 0.015256943132140134\n",
            "Epoch 16/20, Pretraining Loss: 0.013761960852303003\n",
            "Epoch 17/20, Pretraining Loss: 0.012073563607899766\n",
            "Epoch 18/20, Pretraining Loss: 0.010817916867764373\n",
            "Epoch 19/20, Pretraining Loss: 0.009934318938145512\n",
            "Epoch 20/20, Pretraining Loss: 0.008985820741049554\n",
            "Epoch 1/20, Train Loss: 0.968763182037755\n",
            "Epoch 2/20, Train Loss: 0.2521468949210095\n",
            "Epoch 3/20, Train Loss: 0.09516472957636181\n",
            "Epoch 4/20, Train Loss: 0.08644543193574798\n",
            "Epoch 5/20, Train Loss: 0.031327946129941234\n",
            "Epoch 6/20, Train Loss: 0.016380778011424763\n",
            "Epoch 7/20, Train Loss: 0.004733690558503823\n",
            "Epoch 8/20, Train Loss: 0.0011248452206736577\n",
            "Epoch 9/20, Train Loss: 0.00020348857613402958\n",
            "Epoch 10/20, Train Loss: 0.00012959717595980406\n",
            "Epoch 11/20, Train Loss: 0.00011562878200110342\n",
            "Epoch 12/20, Train Loss: 9.350226814843297e-05\n",
            "Epoch 13/20, Train Loss: 9.511260922360969e-05\n",
            "Epoch 14/20, Train Loss: 8.502862702267808e-05\n",
            "Epoch 15/20, Train Loss: 7.62588962587822e-05\n",
            "Epoch 16/20, Train Loss: 7.258323746914117e-05\n",
            "Epoch 17/20, Train Loss: 7.018676296866033e-05\n",
            "Epoch 18/20, Train Loss: 5.2847641870214014e-05\n",
            "Epoch 19/20, Train Loss: 5.9655439575803224e-05\n",
            "Epoch 20/20, Train Loss: 5.337197130996317e-05\n",
            "Test Accuracy: 98.67%\n",
            "Confusion Matrix:\n",
            " [[100   0   0]\n",
            " [  0  99   1]\n",
            " [  2   1  97]]\n"
          ]
        }
      ]
    }
  ]
}